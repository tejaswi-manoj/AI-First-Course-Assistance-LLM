Does it lay eggs?" If yes, go to the subtree dealing with reptiles/birds/fish; if no, go to subtree for mammals, etc. Each path from root to leaf forms a classification rule (like IF conditions AND ... THEN class). Advantages of decision trees in classification:
They are interpretable; the logic is easily understood (transparency: you can trace a decision path).
They can handle heterogeneous data (continuous and categorical features).
They naturally handle feature interactions (each split can involve a different feature, effectively considering interactions non-linearly).
However, unpruned decision trees can overfit (creating too many splits, even on noise). This is mitigated by limiting tree depth, requiring minimum samples per leaf, or pruning. Single decision trees might not be the most accurate classifiers, but they form the basis of powerful ensemble methods like random forests and gradient boosted trees, which often are top performers. During prediction, an instance travels down the tree: at each node, the test is evaluated and the appropriate branch followed, until a leaf is reached, which provides the predicted class (often the majority class among training examples that fell into that leaf). If using probabilities, one might output the fraction of training samples of each class at that leaf. Trees handle multi-class natively. They can also incorporate costs for misclassification by adjusting splitting criteria. They are not very sensitive to feature scaling or normalization (unlike methods like SVM or KNN), since splits are based on relative order or thresholds.
Support Vector Machines
Support Vector Machines (SVMs) are powerful classifiers that find the optimal hyperplane which separates classes with maximum margi
techtarget.com
. In the linear separable case (binary classification), SVM picks the hyperplane (in feature space) that not only separates the two classes but is as far away as possible from the nearest training points of any class (the support vectors). This maximum-margin criterion tends to improve generalization. If the data is not linearly separable, SVM can:
Use soft margins: allow some misclassifications or slack, controlled by a regularization parameter $C$. A smaller $C$ means more tolerance for misclassification (larger margin), a larger