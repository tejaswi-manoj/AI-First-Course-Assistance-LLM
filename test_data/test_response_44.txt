When distance analyzing AI systems from an ethical perspective, its useful to apply traditional ethical frameworks:
Deontological ethics (duty-based ethics): This framework, often associated with Immanuel Kant, focuses on adherence to moral rules or duties. In the context of AI, a deontological approach would stress that an AI must follow certain inviolable rules or principles (for example, respect user privacy or never lie to a human). The systems actions are ethical if they are in line with moral rules, regardless of outcomes. A deontologist might argue that even if an AI could achieve a great benefit through deceit, it should not do so because lying is inherently wrong.
Consequentialist ethics: This approach (with utilitarianism being a major subset) judges actions by their outcomes or consequences. For AI, a consequentialist viewpoint focuses on maximizing overall good or minimizing harm. For instance, a consequentialist AI ethic might weigh the potential benefits vs. harms of deploying a facial recognition system  if it greatly increases security but only slightly infringes on privacy, a utilitarian calculation might favor it (though these calculations are often subjective and complex). The classic utilitarian principle is to achieve the greatest good for the greatest number. In AI, this could translate to algorithms tuned to optimize social welfare metrics, but it raises challenges: who defines the good, and