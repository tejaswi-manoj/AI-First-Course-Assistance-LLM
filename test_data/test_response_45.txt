What features the model has learned (via filter visualizations).
How it responds to specific input (via activation maps highlighting what parts of input trigger certain features).
Structure in the learned representation (via embeddings plots).
For instance, in an image classifier, we might show saliency maps (explained below) or filter visualizations to a user to justify: The network pays attention to these image regions which correspond to the object. For a text model, one might highlight words that most influenced a prediction (like attention weights in a transformer can be visualized to show which words in a sentence the model focused on to translate a particular word, etc.).
Saliency Maps, Grad-CAM
Saliency maps are a way to identify which the features in that image. For example, a simple MLP with 2- inputs, 2 hidden neurons, and 1 output neuron. Indeed, that was one of the simplest proofs of concept that multi-layer perceptrons are strictly more powerful than single-layer. MLPs can be seen as networks that learn progressively more abstract features: The first hidden layer might learn simple features from inputs, the second hidden layer can combine those to learn more complex features, and so on (this is a conceptual view often highlighted in deep learning, e.g., in image processing a DNNs first layer might learn edges, second layer might combine edges into shapes, etc.). In summary, a multi-layer perceptron is a feedforward neural network with one or more hidden layers, allowing it