What local interactions you stack them) combine lower-level features into higher-level patterns (textures, object parts).
Pooling layers interspersed reduce resolution as we go deeper but make features more abstract and global.
Eventually, the last layers might be fully-connected or global average pooling to produce outputs for classification.
For example, a simple CNN for digit recognition might have: conv (5x5 filters, 6 maps) -> ReLU -> max pool (2x2) -> conv (5x5 filters, 16 maps) -> ReLU -> max pool (2x2) -> flatten -> fully connected -> output softmax for 10 digits. This is akin to the classic LeNet-5 architecture.
Key advantages:
Local connectivity: exploits local correlations (in images, pixels close together are more related).
Weight sharing: drastically fewer parameters than full connect (improves generalization and allows scaling to large inputs).
Translation invariance: If a pattern moves in the input, convolution+pooling can still detect it because same filter slides, and pooling abstracts location a bit.
CNNs revolutionized computer vision tasks because they could learn features directly from raw pixels that are far superior to manual features. They are also used for other data types (e.g., 1D CNN for audio, text as sequence of words or characters, etc.). Convolution operations can also be seen as a kind of regularization: a fully connected network could simulate a convolution, but with many more parameters which might overfit. By forcing the receptive fields to be local and weights shared, CNN imposes a prior that the solution should have a locally compositional structure (which matches many natural signals). Pooling may sometimes be replaced or supplemented by other techniques in modern architectures (like strided convolutions, or not pooling at all but using global average at the end). But the concept remains to reduce spatial dimensions as you go deeper, so that final representations are small (like 1x1 per filter in global average pooling, or simply flattened smaller feature maps into fully connected layers). To conclude, CNNs combine convolution (feature extraction with shared weights) and pooling (downsampling) to effectively learn hierarchical features. They form the basis of most image recognition systems, from small models to deep ones like VGG, ResNet, etc., etc., which are essentially very deep CNNs with many conv layers.
17. Explainability in Neural Networks
Interpretability vs Explainability
These terms are related but nuanced. Interpretability usually refers to the extent to which a cause and effect in a system can be observed in understandable terms. Explainability often refers to