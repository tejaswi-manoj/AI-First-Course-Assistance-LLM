What features the model has learned (via filter visualizations).
How it responds to specific input (via activation maps highlighting what parts of input trigger certain features).
Structure in the learned representation (via embeddings plots).
For instance, in an image classifier, we might show saliency maps (explained below) or filter visualizations to a user to justify: The network pays attention to these image regions which correspond to the object. For a text model, one might highlight words that most influenced a prediction (like attention weights in a transformer can be visualized to show which words in a sentence the model focused on to translate a particular word, etc.).
Saliency Maps, Grad-CAM
Saliency maps are a way to identify which pixels of an image (or which features in a general input) most affect the output. A simple saliency method is to take the gradient of the output (say the logit or probability for a class) with respect to the input pixel
sciencedirect.com
. The magnitude of this gradient indicates how much a small change in that pixel would change the output score. By taking absolute or squared gradient and projecting to image shape, you get a heatmap highlighting important pixels. This is essentially the simplest explanation of an image classifiers decision: highlight the pixels that if changed, would most affect the confidence. This often highlights edges of the object or distinctive texture in the object that the model relies on. Saliency maps sometimes are noisy, but are fast to compute. Grad-CAM (Gradient-weighted Class Activation Mapping): Grad-CAM is a specific technique