When iteration than each point, then the decoder starts with a start token and produces outputs step by step. At each step, it uses its self-attention to consider what its generated so far (language model aspect) and encoder-decoder attention to consider relevant words in source. The next word probabilities come from output softmax; you choose the highest or do beam search to generate full output sentence. Encoder-decoder is used beyond translation: any task where an input sequence needs to be transduced into output sequence (summarization, where input is document, output is summary; speech recognition, input audio features, output text sequence; image captioning in a way, input is image encoded by some network, then decode to text; etc.) Even some non-sequential tasks use it: like BERT is just an encoder used for classification or fill-in tasks, but GPT (generative pre-training) is just a decoder (one-directional). T5 is an encoder-decoder transformer for various tasks formulated as text-to-text. So:
Encoder: reads input, builds high-level representation.
Decoder: generates output, attending to those representations.
In Transformers, because of multi-head attention and feed-forward layers, encoders can capture