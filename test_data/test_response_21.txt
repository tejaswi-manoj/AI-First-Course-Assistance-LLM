What features the model has learned (via filter visualizations).
How it responds to specific input (via activation maps highlighting what parts of input trigger certain features).
Structure in the learned representation (via embeddings plots).
For instance, in an image classifier, we might show saliency maps (explained below) or filter visualizations to a user to justify: The network pays embeddings (an array of pixel patches (flattened to vectors), each patch is like a "token". Then a linear projection is applied to each patch to reduce dimensionality