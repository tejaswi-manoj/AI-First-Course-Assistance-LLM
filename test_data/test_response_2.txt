What features the model has learned (via filter visualizations).
How it responds to specific input (via activation maps highlighting what parts of input trigger certain features).
Structure in the learned representation (via embeddings plots).
For instance, in an image classifier, we might show saliency maps (explained below) or filter visualizations to a user to justify: The network pays attention to these image regions which correspond to the object. For a text model, one might highlight words that most influenced a prediction (like attention weights in a transformer can be visualized to show which words in a sentence the model focused on to translate a particular word, etc.).
Saliency Maps, Grad-CAM
Saliency maps are a way to identify which pixels of an image (or which features in a general input) most affect the output. A simple saliency method is to take the gradient of the output (say the logit or probability for a class) with respect to the input pixel
sciencedirect.com
. The magnitude of this gradient indicates how much a small change in that pixel would change the output score. By taking absolute or squared gradient and projecting to image shape, you get a heatmap highlighting important pixels. This is essentially the simplest explanation of an image classifiers decision: highlight the pixels that if changed, would most affect the confidence. This often highlights edges of the object or distinctive texture. This is the object that the model relies on. Saliency maps sometimes are noisy, but are fast to compute. Grad-CAM (Gradient-weighted Class Activation Mapping): Grad-CAM is a specific technique that combines feature maps of a convolutional layer with gradients to produce a localization map for a clas
techtarget.com

techtarget.com
. Specifically, to explain a classification for class $c$, one:
Takes the gradients of the score for class $c$ w.rt. the feature maps of a convolutional layer.
Averages those gradients over the spatial locations to get a weight for each feature map (these weights basically tell how important each filter is for the class).
Then take a weighted sum of the actual feature maps (before ReLU) using those weights.
Apply ReLU to the resulting map (to focus only on positive influences). This produces a heatmap the same size as that convolutional layers feature map (which is smaller than the input, due to pooling/stride). This heatmap is then upsampled to the input size and overlaid on the image. The result highlights the regions in the image that had the strongest influence on the class $c$ outpu
techtarget.com

techtarget.com
.
Grad-CAM is popular because it usually yields more interpretable and localized visual explanations than raw saliency. For example, on an image with a dog and cat, the Grad-CAM for dog might highlight the dogs body, whereas for cat highlights the cat. Other methods:
Layer-wise Relevance Propagation (LRP): backpropagates the prediction backward